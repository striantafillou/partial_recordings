{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST and some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAB6CAYAAAC7kYnCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF4GsG9AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f822437a190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n",
    "\n",
    "# split train data in two parts\n",
    "X_pr = Xtrain[30000:60000, :, :]\n",
    "Y_pr = Ytrain[30000:60000]\n",
    "\n",
    "Xtrain = Xtrain[0:30000, :, :];\n",
    "Ytrain = Ytrain[0:30000]\n",
    "\n",
    "# DOWNSAMPLE THE IMAGES\n",
    "factor = 0.25\n",
    "\n",
    "Xtrain_down = np.ones((Xtrain.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    Xtrain_down[i, :, :] = imresize(Xtrain[i,:,:], factor)\n",
    "\n",
    "Xtest_down = np.ones((Xtest.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtest.shape[0]):\n",
    "    Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n",
    "\n",
    "    \n",
    "X_pr_down = np.ones((X_pr.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(X_pr.shape[0]):\n",
    "    X_pr_down[i,:,:] = imresize(X_pr[i,:,:], factor)\n",
    "\n",
    "    \n",
    "# VECTORIZE IMAGES\n",
    "Xtrain_down = Xtrain_down.reshape(Xtrain.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtest_down  = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "X_pr_down   = X_pr_down.reshape(X_pr.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtrain      = Xtrain.reshape(Xtrain.shape[0], xdim**2).astype('float32') / 255\n",
    "Xtest       = Xtest.reshape(ntest, xdim**2).astype('float32') / 255\n",
    "\n",
    "# Categorical labels\n",
    "Ytrain_cat = np_utils.to_categorical(Ytrain, 10)\n",
    "Ytest_cat = np_utils.to_categorical(Ytest, 10)\n",
    "\n",
    "# VISUALIZATION 20 RANDOM TRAINING SAMPLES\n",
    "# Create 20 subplots\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(10):\n",
    "        axes[i][j].imshow(Xtrain_down[np.random.randint(0, 3000),:].reshape(int(xdim*factor), \n",
    "                          int(ydim*factor)), cmap='gray_r', interpolation='nearest')\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN A FULLY-CONNECTED NN WITH 4 \"LINEAR\" HIDDEN LAYERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.4972 - acc: 0.8487     \n",
      "Epoch 2/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.4024 - acc: 0.8801     \n",
      "Epoch 3/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3936 - acc: 0.8850     \n",
      "Epoch 4/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3903 - acc: 0.8842     \n",
      "Epoch 5/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3873 - acc: 0.8849     \n",
      "Epoch 6/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3847 - acc: 0.8864     \n",
      "Epoch 7/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3799 - acc: 0.8881     \n",
      "Epoch 8/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3822 - acc: 0.8893     \n",
      "Epoch 9/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3765 - acc: 0.8907     \n",
      "Epoch 10/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3725 - acc: 0.8922     \n",
      "Epoch 11/50\n",
      "30000/30000 [==============================] - 4s - loss: 0.3757 - acc: 0.8909     \n",
      "Epoch 12/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3720 - acc: 0.8907     \n",
      "Epoch 13/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3711 - acc: 0.8918     \n",
      "Epoch 14/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3695 - acc: 0.8906     \n",
      "Epoch 15/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3674 - acc: 0.8913     \n",
      "Epoch 16/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3664 - acc: 0.8930     \n",
      "Epoch 17/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3647 - acc: 0.8928     \n",
      "Epoch 18/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3639 - acc: 0.8936     \n",
      "Epoch 19/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3642 - acc: 0.8938     \n",
      "Epoch 20/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3608 - acc: 0.8940     \n",
      "Epoch 21/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3632 - acc: 0.8933     \n",
      "Epoch 22/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3636 - acc: 0.8924     \n",
      "Epoch 23/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3613 - acc: 0.8936     \n",
      "Epoch 24/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3613 - acc: 0.8953     \n",
      "Epoch 25/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3585 - acc: 0.8947     \n",
      "Epoch 26/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3572 - acc: 0.8950     \n",
      "Epoch 27/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3582 - acc: 0.8955     \n",
      "Epoch 28/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3567 - acc: 0.8953     \n",
      "Epoch 29/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3559 - acc: 0.8954     \n",
      "Epoch 30/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3562 - acc: 0.8975     \n",
      "Epoch 31/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3578 - acc: 0.8946     \n",
      "Epoch 32/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3557 - acc: 0.8959     \n",
      "Epoch 33/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3538 - acc: 0.8979     \n",
      "Epoch 34/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3553 - acc: 0.8955     \n",
      "Epoch 35/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3534 - acc: 0.8968     \n",
      "Epoch 36/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3520 - acc: 0.8952     \n",
      "Epoch 37/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3538 - acc: 0.8968     \n",
      "Epoch 38/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3515 - acc: 0.8971     \n",
      "Epoch 39/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3525 - acc: 0.8970     \n",
      "Epoch 40/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3498 - acc: 0.8990     \n",
      "Epoch 41/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3508 - acc: 0.8957     \n",
      "Epoch 42/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3518 - acc: 0.8991     \n",
      "Epoch 43/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3518 - acc: 0.8976     \n",
      "Epoch 44/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3508 - acc: 0.8961     \n",
      "Epoch 45/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3499 - acc: 0.8992     \n",
      "Epoch 46/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3510 - acc: 0.8974     \n",
      "Epoch 47/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3482 - acc: 0.8992     \n",
      "Epoch 48/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3481 - acc: 0.8990     \n",
      "Epoch 49/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3499 - acc: 0.8980     \n",
      "Epoch 50/50\n",
      "30000/30000 [==============================] - 5s - loss: 0.3485 - acc: 0.8985     \n",
      "\n",
      "acc: 90.29%\n",
      "[array([[-0.01419207, -0.06122095,  0.00616312, ...,  0.03373057,\n",
      "        -0.03521595,  0.15167499],\n",
      "       [-0.0438939 , -0.01699977,  0.01365642, ...,  0.03409437,\n",
      "        -0.01169823, -0.07749701],\n",
      "       [-0.00781658, -0.01111274,  0.00257853, ...,  0.03967579,\n",
      "        -0.04006007,  0.15336883],\n",
      "       ..., \n",
      "       [-0.05026852,  0.14229226,  0.00320209, ..., -0.04154385,\n",
      "         0.00547614,  0.12861475],\n",
      "       [-0.04274553,  0.13099515,  0.01792208, ...,  0.06920587,\n",
      "        -0.01089876, -0.17946027],\n",
      "       [ 0.00342035,  0.02428952,  0.01050691, ..., -0.0381237 ,\n",
      "         0.0025529 ,  0.04140291]], dtype=float32), array([[  2.48673633e-02,   1.29065573e+00,  -6.74256962e-03, ...,\n",
      "          2.13038288e-02,   1.98090095e-02,  -3.62590067e-02],\n",
      "       [  3.45614813e-02,   1.52356470e+00,   2.56764255e-02, ...,\n",
      "          3.03661637e-03,  -1.17974784e-02,  -2.87512243e-02],\n",
      "       [  1.22232940e-02,   7.95540154e-01,   2.23158486e-03, ...,\n",
      "         -1.70251112e-02,   2.71073990e-02,  -3.64309177e-04],\n",
      "       ..., \n",
      "       [  2.10156627e-02,   1.09245443e+00,   2.70443112e-02, ...,\n",
      "          4.08228561e-02,   1.06123537e-02,  -2.19672509e-02],\n",
      "       [ -3.59690115e-02,  -6.80781186e-01,  -1.26328981e-02, ...,\n",
      "          4.08796817e-02,  -3.76942381e-02,   1.30355712e-02],\n",
      "       [  6.16494827e-02,   4.92084831e-01,   5.77358715e-02, ...,\n",
      "          3.75806503e-02,  -1.26207136e-02,   8.52085650e-04]], dtype=float32), array([[-0.91576117,  0.04159969, -0.10071727, ..., -0.04304371,\n",
      "        -0.04789013, -0.01474151],\n",
      "       [-0.0095064 ,  0.05082555, -0.14615077, ..., -0.07149599,\n",
      "        -0.11061494,  0.03551074],\n",
      "       [-1.52395248, -0.02346424, -0.04118017, ...,  0.0144446 ,\n",
      "         0.02701571, -0.06375512],\n",
      "       ..., \n",
      "       [ 0.97010165,  0.06240959, -0.17090729, ..., -0.08092908,\n",
      "        -0.08028337,  0.04360474],\n",
      "       [-0.89336103, -0.09717275, -0.05837739, ...,  0.05191644,\n",
      "         0.11266365, -0.01357194],\n",
      "       [-0.40071598,  0.0601281 , -0.10426853, ..., -0.04209404,\n",
      "        -0.0866181 ,  0.05038336]], dtype=float32), array([[  6.86663166e-02,   1.45150161e+00,   1.60190439e+00, ...,\n",
      "          9.15621221e-02,  -1.47366405e-01,   1.12526238e+00],\n",
      "       [ -6.95518870e-03,   1.22520328e-03,   8.54373574e-02, ...,\n",
      "          1.00408360e-01,  -2.30448499e-01,  -5.22013068e-01],\n",
      "       [ -7.70540461e-02,   2.20965290e+00,   2.11230540e+00, ...,\n",
      "          9.20792520e-02,   8.21923651e-03,   9.68609691e-01],\n",
      "       ..., \n",
      "       [  1.04573555e-01,   7.36453116e-01,   1.92727780e+00, ...,\n",
      "         -1.90719627e-02,   2.72910018e-02,   2.78542787e-01],\n",
      "       [ -6.01385683e-02,   1.56393993e+00,  -1.61544633e+00, ...,\n",
      "         -8.66311491e-02,   2.34461635e-01,   1.69969201e+00],\n",
      "       [ -3.61506715e-02,  -1.63236976e-01,   1.35974741e+00, ...,\n",
      "          2.40488164e-03,   4.79983985e-02,   6.02778792e-01]], dtype=float32), array([[  2.90250864e-06,   6.68649955e-05,   5.77697589e-04, ...,\n",
      "          1.09451968e-04,   1.52013148e-03,   3.01877481e-05],\n",
      "       [  1.47012213e-06,   1.78334574e-08,   5.73593525e-06, ...,\n",
      "          9.95645702e-01,   1.91685867e-05,   4.16352693e-03],\n",
      "       [  7.55249971e-07,   4.78034906e-07,   6.46035405e-05, ...,\n",
      "          2.85396482e-07,   2.05947831e-03,   1.20705183e-04],\n",
      "       ..., \n",
      "       [  9.13577787e-06,   2.54540328e-06,   7.11469966e-06, ...,\n",
      "          1.41192853e-04,   9.54339653e-03,   6.04537316e-04],\n",
      "       [  3.70865711e-03,   1.68684096e-06,   2.28455546e-03, ...,\n",
      "          1.96576384e-05,   6.84460625e-04,   4.10639186e-05],\n",
      "       [  1.51930628e-02,   2.11003976e-06,   5.08393301e-03, ...,\n",
      "          4.50209901e-03,   7.96012938e-01,   3.37754115e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(int(xdim*factor)**2,)))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "with with tf.device('/gpu:0'):\n",
    "    model.fit(Xtrain_down, Ytrain_cat, nb_epoch=50, batch_size=32)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(Xtest_down, Ytest_cat, verbose=0)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# # calculate predictions\n",
    "# Ypredict = model.predict(Xtest)\n",
    "# # round predictions\n",
    "# rounded = [round(x[0]) for x in Ypredict]\n",
    "# print(rounded)\n",
    "\n",
    "# Save the model\n",
    "model.save('linear_nn.h5')\n",
    "\n",
    "# GET THE OUTPUT OF EACH LAYER AFTER TRAINING\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "layer_outs = functor([X_pr_down, 1.])                       # compute on 2nd training set\n",
    "layer_outs_test = functor([Xtest_down, 1.])                 # compute on test set\n",
    "#print(layer_outs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subsample and do xgboost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'layer_outs_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-5e8d325090d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m#print('#neuron, #iteratin, subnetsize: ', iN,it,subnetSize[ss])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mxg_train\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_subsample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_subsample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mxg_test\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_outs_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_outs_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mwatchlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxg_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxg_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# train XGboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer_outs_test' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "from RE_PartialRecData import RE_PartialRecData\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import pickle\n",
    "\n",
    "params = {}\n",
    "# use softmax multi-class classification 'multi:softmax'\n",
    "# use linear regression 'reg:linear'\n",
    "params['objective'] = 'reg:linear'\n",
    "# scale weight of positive examples\n",
    "params['eta'] = 0.4\n",
    "params['max_depth'] = 6\n",
    "params['silent'] = 1\n",
    "params['nthread'] = 4\n",
    "# params['num_class'] = 10\n",
    "num_round=5\n",
    "\n",
    "# how many recordings?\n",
    "nRecordings = 10\n",
    "# how many neurons from the firs hidden layer?\n",
    "subnetSize = [2**x for x in range(8)]\n",
    "nSubnetSize = len(subnetSize)\n",
    "# which layers?\n",
    "# how many samples per recording?\n",
    "nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)\n",
    "# how many iterations\n",
    "nIterations = 50\n",
    "\n",
    "# baseline prediction error\n",
    "#bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "oLayer = len(layer_outs)-1  # index of output layer\n",
    "nOutNeurons = layer_outs[oLayer].shape[1]\n",
    "rmses = np.zeros([nIterations, nOutNeurons, nSubnetSize])\n",
    "\n",
    "for ss in range(nSubnetSize):\n",
    "    nLayerNeurons = [subnetSize[ss], 0, 0, 0, 10]\n",
    "    print(subnetSize[ss])\n",
    "    for it in range(nIterations):\n",
    "        # copy data\n",
    "        layer_outputs = deepcopy(layer_outs)\n",
    "        # subsample\n",
    "        X_subsample, Y_subsample = RE_PartialRecData(layer_outputs, nLayerNeurons, nRecordings, nSamples[ss])\n",
    "        #print('# nan neurons: ',np.count_nonzero(np.isnan(X_subsample[:3000,:]).sum(axis=0)))\n",
    "        # prepare data for xgboost\n",
    "        for iN in range(nOutNeurons):\n",
    "            #print('#neuron, #iteratin, subnetsize: ', iN,it,subnetSize[ss])\n",
    "            xg_train  = xgb.DMatrix(X_subsample, label=Y_subsample[:, iN])\n",
    "            xg_test   = xgb.DMatrix(layer_outs_test[0], label=layer_outs_test[3][:,iN])\n",
    "            watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "            # train XGboost\n",
    "            bst = xgb.train(params, xg_train, num_round, watchlist, verbose_eval=True)\n",
    "            # get predictions\n",
    "            pred = bst.predict(xg_test)\n",
    "            rmses[it,iN,ss] = np.sqrt(np.mean(np.square([(pred[i] - layer_outs_test[3][:,iN][i]) \n",
    "                                         for i in range(len(layer_outs_test[3][:,1]))])))\n",
    "            #print ('predicting, RMSE=%f' %rmses[it, iN, ss])\n",
    "            \n",
    "\n",
    "# save the rmse's\n",
    "with open('RMSE_Layer1_Linear.dat','wb') as f:\n",
    "    pickle.dump(rmses, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
