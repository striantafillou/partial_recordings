{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "# Load MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n",
    "\n",
    "# split train data in two\n",
    "X_pr = Xtrain[30000:60000, :, :]\n",
    "Y_pr = Ytrain[30000:60000]\n",
    "\n",
    "Xtrain = Xtrain[0:30000, :, :];\n",
    "Ytrain = Ytrain[0:30000]\n",
    "\n",
    "# downsample\n",
    "factor = 0.25\n",
    "\n",
    "Xtrain_down = np.ones((Xtrain.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    Xtrain_down[i, :, :] = imresize(Xtrain[i,:,:], factor)\n",
    "\n",
    "Xtest_down = np.ones((Xtest.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtest.shape[0]):\n",
    "    Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n",
    "\n",
    "X_pr_down = np.ones((X_pr.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(X_pr.shape[0]):\n",
    "    X_pr_down[i,:,:] = imresize(X_pr[i,:,:], factor)\n",
    "    \n",
    "# # *** VECTORIZE IMAGES ***\n",
    "Xtrain_down = Xtrain_down.reshape(Xtrain_down.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtest_down = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "X_pr_down = X_pr_down.reshape(X_pr_down.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "\n",
    "Ytrain_cat = np_utils.to_categorical(Ytrain, 10)\n",
    "Ytest_cat = np_utils.to_categorical(Ytest, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.4526 - acc: 0.8623     \n",
      "Epoch 2/50\n",
      "30000/30000 [==============================] - 6s - loss: 0.2237 - acc: 0.9292     \n",
      "Epoch 3/50\n",
      "30000/30000 [==============================] - 6s - loss: 0.1769 - acc: 0.9438     \n",
      "Epoch 4/50\n",
      "30000/30000 [==============================] - 6s - loss: 0.1527 - acc: 0.9518     \n",
      "Epoch 5/50\n",
      "30000/30000 [==============================] - 6s - loss: 0.1353 - acc: 0.9569     \n",
      "Epoch 6/50\n",
      "14608/30000 [=============>................] - ETA: 3s - loss: 0.1186 - acc: 0.9613"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# *** TRAIN A FULLY-CONNECTED NN WITH THREE HIDDEN LAYERS ***\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(int(xdim*factor)**2,), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "with tf.device('/gpu:1'):\n",
    "    model.fit(Xtrain_down, Ytrain_cat, nb_epoch=50, batch_size=16)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(Xtest_down, Ytest_cat, verbose=0)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "layer_outs = functor([X_pr_down, 1.])\n",
    "\n",
    "import pickle\n",
    "with open('layer_outs.dat','wb') as f:\n",
    "    pickle.dump(layer_outs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7334c2f3d0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# how many samples per recording?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mnSamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pr_down\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnRecordings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubnetSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m# how many iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mnIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# subsample and do xgboost regression\n",
    "from copy import copy, deepcopy\n",
    "from RE_PartialRecData import RE_PartialRecData\n",
    "\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "params = {}\n",
    "# use softmax multi-class classification 'multi:softmax'\n",
    "# use linear regression 'reg:linear'\n",
    "params['objective'] = 'reg:linear'\n",
    "# scale weight of positive examples\n",
    "params['eta'] = 0.4\n",
    "params['max_depth'] = 5\n",
    "params['silent'] = 1\n",
    "params['nthread'] = 4\n",
    "# params['num_class'] = 10\n",
    "num_round=5\n",
    "\n",
    "\n",
    "# how many recordings?\n",
    "nRecordings = 10\n",
    "# how many neurons from the firs hidden layer?\n",
    "subnetSize = range(1,100,5)\n",
    "nSubnetSize = len(subnetSize)\n",
    "# which layer?\n",
    "iLayer=0;\n",
    "# how many samples per recording?\n",
    "#nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)\n",
    "nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)*100\n",
    "# how many iterations\n",
    "nIterations = 50\n",
    "\n",
    "# baseline prediction error\n",
    "#bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "oLayer = len(layer_outs)-1  # index of output layer\n",
    "nOutNeurons = layer_outs[oLayer].shape[1]\n",
    "rmses = np.zeros([nIterations, nOutNeurons, nSubnetSize])\n",
    "\n",
    "for ss in range(nSubnetSize):\n",
    "    nLayerNeurons = subnetSize[ss]\n",
    "    print(subnetSize[ss])\n",
    "    for it in range(nIterations):\n",
    "        # copy data\n",
    "        layer_outputs = deepcopy(layer_outs)\n",
    "        # subsample\n",
    "        X_subsample, Y_subsample = RE_PartialRecData2(layer_ouputs[iLayer], layer_outputs[oLayer], nLayerNeurons, nRecordings, nSamples[ss])\n",
    "        #print('# nan neurons: ',np.count_nonzero(np.isnan(X_subsample[:3000,:]).sum(axis=0)))\n",
    "        # prepare data for xgboost\n",
    "        for iN in range(nOutNeurons):\n",
    "            #print('#neuron, #iteratin, subnetsize: ', iN,it,subnetSize[ss])\n",
    "            xg_train  = xgb.DMatrix(X_subsample, label=Y_subsample[:, iN])\n",
    "            xg_test   = xgb.DMatrix(layer_outs_test[0], label=layer_outs_test[3][:,iN])\n",
    "            watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "            # train XGboost\n",
    "            bst = xgb.train(params, xg_train, num_round, watchlist, verbose_eval=False)\n",
    "            # get predictions\n",
    "            pred = bst.predict(xg_test)\n",
    "            rmses[it,iN,ss] = np.sqrt(np.mean(np.square([(pred[i] - layer_outs_test[3][:,iN][i]) \n",
    "                                         for i in range(len(layer_outs_test[3][:,1]))])))\n",
    "            #print ('predicting, RMSE=%f' %rmses[it, iN, ss])\n",
    "\n",
    "            \n",
    "\n",
    "# save the rmse's\n",
    "import pickle\n",
    "\n",
    "with open('RMSE_layer1.dat','wb') as f:\n",
    "    pickle.dump(rmses, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('RMSE_layer1.dat','r') as f:\n",
    "    rmses = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "fig=pl.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xlim([0, 100])\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "x = subnetSize;\n",
    "y = np.mean(np.mean(rmses, axis=1), axis=0)\n",
    "error = np.std(np.mean(rmses, axis=1), axis=0)\n",
    "bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "pl.plot(x, y, 'k-')\n",
    "horiz_line_data = np.array([bl for i in xrange(len(x))])\n",
    "pl.plot(x, horiz_line_data, 'k--') \n",
    "pl.fill_between(x, y-error, y+error, alpha=0.2, facecolor='#808080')\n",
    "\n",
    "ax1.set_xlabel('# observed neurons on Layer 1 (out of 100)', fontsize=18)\n",
    "ax2.set_xlabel('Samples per recording',  fontsize=16)\n",
    "\n",
    "new_tick_locations =range(1, 100, 20)\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.set_xticks(new_tick_locations)\n",
    "ax2.set_xticklabels(nSamples[range(0, 20, 4)])\n",
    "\n",
    "\n",
    "# ax2.set_xticks(nSamples)\n",
    "# ax2.set_xticklabels(nSamples[range(0, 5, 20)])\n",
    "ax1.set_ylabel('RMSE', fontsize=18)\n",
    "ax1.set_ylim([0, .35])\n",
    "\n",
    "pl.text(90,0.3, 'baseline')\n",
    "\n",
    "# #pl.title('%d recordings'%(nRecordings), fontsize=18)\n",
    "# #pl.title(nRecordings ' recordings ')\n",
    "#pl.show()\n",
    "pl.draw()\n",
    "pl.savefig('RMSEs_Layer_1.pdf', format='pdf')#, dpi=1000)\n",
    "pl.savefig('RMSEs_Layer_1.eps', format='eps', dpi=1000)\n",
    "pl.savefig('RMSEs_Layer_1.png', format='png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subsample and do xgboost regression\n",
    "from copy import copy, deepcopy\n",
    "from RE_PartialRecData import RE_PartialRecData\n",
    "\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import time\n",
    "\n",
    "\n",
    "params = {}\n",
    "# use softmax multi-class classification 'multi:softmax'\n",
    "# use linear regression 'reg:linear'\n",
    "params['objective'] = 'reg:linear'\n",
    "# scale weight of positive examples\n",
    "params['eta'] = 0.4\n",
    "params['max_depth'] = 5\n",
    "params['silent'] = 1\n",
    "params['nthread'] = 4\n",
    "# params['num_class'] = 10\n",
    "num_round=5\n",
    "# how many recordings?\n",
    "nRecordings = 10\n",
    "# how many neurons from the firs hidden layer?\n",
    "subnetSize = range(1,100,5)\n",
    "nSubnetSize = len(subnetSize)\n",
    "# which layers?\n",
    "# how many samples per recording?\n",
    "nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)\n",
    "# how many iterations\n",
    "nIterations = 50\n",
    "\n",
    "# baseline prediction error\n",
    "#bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "oLayer = len(layer_outs)-1  # index of output layer\n",
    "nOutNeurons = layer_outs[oLayer].shape[1]\n",
    "rmses = np.zeros([nIterations, nOutNeurons, nSubnetSize])\n",
    "\n",
    "for ss in range(nSubnetSize):\n",
    "    nLayerNeurons = [0, subnetSize[ss], 0, 10]\n",
    "    start = time.time()\n",
    "\n",
    "    for it in range(nIterations):\n",
    "        # copy data\n",
    "        layer_outputs = deepcopy(layer_outs)\n",
    "        # subsample\n",
    "        X_subsample, Y_subsample = RE_PartialRecData(layer_outputs, nLayerNeurons, nRecordings, nSamples[ss])\n",
    "        #print('# nan neurons: ',np.count_nonzero(np.isnan(X_subsample[:3000,:]).sum(axis=0)))\n",
    "        # prepare data for xgboost\n",
    "        for iN in range(nOutNeurons):\n",
    "            #print('#neuron, #iteratin, subnetsize: ', iN,it,subnetSize[ss])\n",
    "            xg_train  = xgb.DMatrix(X_subsample, label=Y_subsample[:, iN])\n",
    "            xg_test   = xgb.DMatrix(layer_outs_test[1], label=layer_outs_test[3][:,iN])\n",
    "            watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "            # train XGboost\n",
    "            bst = xgb.train(params, xg_train, num_round, watchlist, verbose_eval=False)\n",
    "            # get predictions\n",
    "            pred = bst.predict(xg_test)\n",
    "            rmses[it,iN,ss] = np.sqrt(np.mean(np.square([(pred[i] - layer_outs_test[3][:,iN][i]) \n",
    "                                         for i in range(len(layer_outs_test[3][:,1]))])))\n",
    "            # run your code\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(subnetSize[ss],elapsed,X_subsample.shape[0])\n",
    "\n",
    "# save the rmse's\n",
    "import pickle\n",
    "with open('RMSE_layer2.dat','wb') as f:\n",
    "    pickle.dump(rmses, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('RMSE_layer2.dat','r') as f:\n",
    "    rmses = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "fig=pl.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xlim([0, 100])\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "x = subnetSize;\n",
    "y = np.mean(np.mean(rmses, axis=1), axis=0)\n",
    "error = np.std(np.mean(rmses, axis=1), axis=0)\n",
    "bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "pl.plot(x, y, 'k-')\n",
    "horiz_line_data = np.array([bl for i in xrange(len(x))])\n",
    "pl.plot(x, horiz_line_data, 'k--') \n",
    "pl.fill_between(x, y-error, y+error, alpha=0.2, facecolor='#808080')\n",
    "\n",
    "ax1.set_xlabel('# observed neurons on Layer 2 (out of 100)', fontsize=16)\n",
    "ax2.set_xlabel('Samples per recording',  fontsize=18)\n",
    "\n",
    "new_tick_locations =range(1, 100, 20)\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.set_xticks(new_tick_locations)\n",
    "ax2.set_xticklabels(nSamples[range(0, 20, 4)])\n",
    "\n",
    "\n",
    "# ax2.set_xticks(nSamples)\n",
    "# ax2.set_xticklabels(nSamples[range(0, 5, 20)])\n",
    "ax1.set_ylabel('RMSE', fontsize=18)\n",
    "ax1.set_ylim([0, .35])\n",
    "\n",
    "pl.text(90,0.3, 'baseline')\n",
    "\n",
    "# #pl.title('%d recordings'%(nRecordings), fontsize=18)\n",
    "# #pl.title(nRecordings ' recordings ')\n",
    "pl.draw()\n",
    "pl.savefig('RMSEs_Layer_2.pdf', format='pdf')#, dpi=1000)\n",
    "pl.savefig('RMSEs_Layer_2.png', format='png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nSamples[[1, 6, 10]]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
