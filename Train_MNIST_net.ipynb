{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *** DOWNSAMPLE THE IMAGES ***\n",
    "factor = 1/4\n",
    "\n",
    "Xtrain_down = np.ones((ntrain, int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(ntrain):\n",
    "    Xtrain_down[i, :, :] = imresize(Xtrain[i,:,:], factor)\n",
    "\n",
    "Xtest_down = np.ones((ntest, int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(ntest):\n",
    "    Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *** VECTORIZE IMAGES ***\n",
    "Xtrain_down = Xtrain_down.reshape(ntrain, int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtest_down = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtrain = Xtrain.reshape(ntrain, xdim**2).astype('float32') / 255\n",
    "Xtest = Xtest.reshape(ntest, xdim**2).astype('float32') / 255\n",
    "# Categorical labels\n",
    "Ytrain = np_utils.to_categorical(Ytrain, 10)\n",
    "Ytest = np_utils.to_categorical(Ytest, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAACqCAYAAAAqYjm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAFsxJREFUeJzt3WlsVPX3x/Ez0JUWwZZFAlVEQapBBNGIURAQ0AQxCi5B\nEI1rjJq4a0SjuCbuGlwwGFlEfUTEoCIaFTVuUVSIBUGtFKkK2LJ1o+39PRAJ+Sed8+ntdzrT/t+v\nZB7Np+cOp3fmzmFgTiKKIgMAAACAkLqk+wEAAAAA6HwYNAAAAAAEx6ABAAAAIDgGDQAAAADBMWgA\nAAAACI5BAwAAAEBwDBoAAAAAgmPQAAAAABBclhJKJBLFZjbZzMrNrC6VDyjD5ZnZQDNbGUXRDuUH\n6J2ZxeibGb3bj3MuPnoXH72Lj97FR+/i4RobH+dcfFrvoihyb2Y2w8wibgduM5S+0bv4faN38XtH\n3+gdvcuIG72jdxnbN3oXv3f0rXW9kz7RsH+nNluyZImVlpaKP9J29fX1bqayslKq1a9fPzeTm5ub\n9P6ysjKbOXOm2f5+iMrNwvWusbHRzXTpov2LuEQiESTjidm3A/n2Pu8ySSacc01NTcFyOTk5bX04\nskzonUrpnfp7CNHjjtS7TNPZeldVVSXl8vLy3Ex+fn7S+ztb73bu3Olm/vrrL6nW4MGDW7xv/fr1\nGXGN3f8mPKnm5uZgtbKy1LewLets51x7Unun/pbqzMxKS0tt5MiRbXtkrVBX538iVVRUJNU64ogj\n3IzyQrlfaz4qC9q7jjhoHKS1HzGm5bzLUGk759Q3uMq56Q3zKZK23qmUHiv9NQve44zvXQbrFL3b\nvn27lPOGCDOzgoIC9bCdonf//POPm6moqJBqHX/88UosrdfYjjhoHKRTnHNpkrR3/GdwAAAAAMEx\naAAAAAAIjkEDAAAAQHAMGgAAAACCY9AAAAAAEFzQ/7KvUr9l4frrr3czy5cvl2q9/fbbbmbKlClS\nrVRQvmHBzGz+/PlupqSkRKqlfDvNpEmTpFqdxcaNG6XcV1995WZ69uwp1RowYECL923YsEGqEdeu\nXbvczAMPPCDVKisrczNz586VanWWb/JQv7Hn3nvvdTPff/+9VEv5fY0fPz7p/errUSrt2bNHyr3/\n/vtu5qeffpJqXXLJJW7myCOPlGp1BLt373Yz06dPl2rdf//9bmbs2LFSrUynPj/eeOMNN6N+s6P4\nrVMpoX5T1IoVK9zM559/LtVSnv933HGHVEt9T5Qu69evl3ILFixwM4ceeqhU65prrnEzxcXFUi0P\nn2gAAAAACI5BAwAAAEBwDBoAAAAAgmPQAAAAABAcgwYAAACA4Bg0AAAAAATHoAEAAAAgOAYNAAAA\nAMExaAAAAAAILi2bwfPz86XcBRdc4GbWrl0r1erTp4+US5eGhgYp98EHH7iZiy++WKr12muvuZkx\nY8ZItfLy8qRcOn377bdu5pxzzpFqDR8+3M2ce+65Uq1kG8RramqkGnEpm8fVbekjRoxwM++8845U\nq7NsBv/000+l3BdffOFm1M3g8+fPdzPe87qpqUk6VlzKxvSHH35YqqVswv3ss8+kWsr25Y6wGVzd\nXP3MM8+4mYqKCqnWCSecIOUy3YcffuhmamtrpVrKNeehhx6SaiXbIK5uF49LfZ912223uZmpU6dK\ntebNm+dm/v77b6nWokWLWryvvr5eqhGX8t7upptukmoNGTLEzSjvEc3Mxo0b52ZGjx4t1fLwiQYA\nAACA4Bg0AAAAAATHoAEAAAAgOAYNAAAAAMExaAAAAAAIjkEDAAAAQHAMGgAAAACCY9AAAAAAEFxa\nFvZ98sknUm7WrFluZuzYsVKtE088Ucqly9dffy3ltm3b5mYKCwulWt98802QjFnyZW2pXjqnWr16\ntZtpbm6WanXv3t3NXHHFFVKt7OzsFu/r1q2bVCOu4447zs08+OCDUq2FCxe6mWTLCTua3bt3u5mf\nf/5ZqqUs47vwwgulWnPmzHEzyc45M7OsrNReGt577z03o76OKcsF33//fanW1VdfLeUy3aZNm6Tc\nPffc42bUJZs9evSQcumi9uS8885zM8pz38xs6NChbkZdrphOb731lpRTFiMr55yZWZcu/t+DK68j\nZsmX5u3bt0+qEZfy5ygpKZFqPfvss25GWTpqZlZaWirlQuATDQAAAADBMWgAAAAACI5BAwAAAEBw\nDBoAAAAAgmPQAAAAABAcgwYAAACA4Bg0AAAAAATHoAEAAAAgOAYNAAAAAMEFX/+qbP2uqqqSak2d\nOtXNFBQUSLW6du0q5VKlsbEx6QbKF154QaqzZcsWN/Piiy9KtX799Vc3s2zZMqlW3759W7xP2Wbe\nFlu3bpVy9fX1bqaoqKitD+cAZSNouimbx9XN7o8//ribueuuu6RayrbnVD6nGxoa3PNl3rx5wY43\nbNgwN3PNNdcEq5VuRx11lJtZvXq1VOvNN990M+PHj5dqTZ48Wcqlk7LJ+Pbbb5dqTZw40c2ceeaZ\nUq10i6Io6Zbtjz/+WKqjnCsrVqyQaj366KNupnfv3lKtdKqurpZyvXr1cjO7du2SaiUSCTdz6aWX\nSrUKCwtbvE+5BrZFVpb/NvvGG2+Uai1cuNDNPPHEE1Ktnj17SrkQMv+dEAAAAIAOh0EDAAAAQHAM\nGgAAAACCY9AAAAAAEByDBgAAAIDgGDQAAAAABMegAQAAACA4Bg0AAAAAwQVf2LdmzRo3oyyxMTMb\nMmSIm7nvvvukWunWtWvXpItb1J706NHDzWRnZ0u1Nm7c6GaGDh0q1crNzW3xvj179kg14qqoqJBy\nyrI4dWHXk08+6WbSvSQylL///lvKXXbZZW5GeX0w05ZJDh48WKoVR5cuXdyFi0cffbRbZ926dfLx\nPKtWrZJqjRs3zs0oy7BS6ZRTTnEzZWVlUq0dO3a4GXUharLFXpnixx9/dDMrV66UailLEdXrSbol\nEomk5/WsWbOkOnV1dW7m/PPPl2opS4fT/VxUjBw5Usopz7MZM2ZItSZMmOBmrrjiCqlWstfXVC/W\nTbZE8j+LFy+Was2ePdvNnHHGGVKt9sQnGgAAAACCY9AAAAAAEByDBgAAAIDgGDQAAAAABMegAQAA\nACA4Bg0AAAAAwTFoAAAAAAiOQQMAAABAcAwaAAAAAIILvhn86quvdjOTJk2SavXv39/NKJuyM4G3\ntfTwww9vx0fzr+HDh7f7MVPh5JNPlnJ79+51M8k2nB+ss2z9Vqjb0idOnOhmfv/9d6lW3759pVyq\nZGVluRuRp02b5tYZM2aMdLxRo0a5maOOOkqq1RE2DTc1NbmZmpoaqdbjjz/uZvr06SPV6giUjfRr\n166Vag0aNKitD6fDUF/br732Wjejvv53hOeiYvr06VKuV69ebkZ9Lg4bNszNdISt9cprXUlJiVTr\nuuuuczNZWcHf1rcZn2gAAAAACI5BAwAAAEBwDBoAAAAAgmPQAAAAABAcgwYAAACA4Bg0AAAAAATH\noAEAAAAgOPULd/PMzMrKytxgbW2tm6msrJQOum3bNjfTvXt3qVYIB/3581rxY3LvOquYfTuQ93oX\nRZFUrK6uzs3k5ORItdprj0YmnHONjY1Srrm52c2oz/0///zTzRQWFia9P9W9U867qqoq6aCbNm1y\nM/X19VIt5TXYk+reKedURUWFdNB169a5GfX3EEKqe7d79243U11dLR1UybXnLoiO8nqXaXs0Un2N\nVV97lNexHTt2SLWU30OInRGd7bVO7W8Icu+iKHJvZjbDzCJuB24zlL7Ru/h9o3fxe0ff6B29y4gb\nvaN3Gds3ehe/d/Stdb1LKH8rl0gkis1sspmVm5n/18KdV56ZDTSzlVEUSWMjvTOzGH0zo3f7cc7F\nR+/io3fx0bv46F08XGPj45yLT+qdNGgAAAAAQGvwn8EBAAAABMegAQAAACA4Bg0AAAAAwTFoAAAA\nAAiOQQMAAABAcAwaAAAAAIJj0AAAAAAQHIMGAAAAgOAYNAAAAAAEx6ABAAAAIDgGDQAAAADBMWgA\nAAAACI5BAwAAAEBwDBoAAAAAgmPQAAAAABBclhJKJBLFZjbZzMrNrC6VDyjD5ZnZQDNbGUXRDuUH\n6J2ZxeibGb3bj3MuPnoXH72Lj97FR+/i4RobH+dcfFrvoihyb2Y2w8wibgduM5S+0bv4faN38XtH\n3+gdvcuIG72jdxnbN3oXv3f0rXW9kz7RsH+nNluyZImVlpaKP9I+duzQhvesLP+P2qNHj6T3l5WV\n2cyZM83290NUbhaud/tP8qTq6rQBu7m52c1069ZNqpVIJFq8L2bfDuQz8byrr693M3v37pVqFRUV\ntXhfqs855XxS/qxmZl27dnUz+/btC1YrNzc36f2Z8HztqOhdfJ2td01NTVKuSxf/X2Inu06YZUbv\nlOtiWVmZVGvgwIFupqCgQKqVTEe6xm7evFnKff/9925m8uTJUq1k14pMOOdC2rp1q5RTzvMBAwYk\nvV/tnTpo1JmZlZaW2siRI8UfaR9//fWXlMvOznYzyd7w/R+t+agsaO+UN4Y1NTVSLeVEKywslGp5\nF5D9WvsRY8aed8owt2vXLqlWnz59pENKxQ7KKn0LObgqw0FDQ4NUS/mLgby8PKmWpfH52gnQu/g6\nRe/ac9A4SNp6p1wXlddNM7OhQ4e6me7du0u1RBl/jVX/vNXV1W5mxIgRUi3xWtEpnq/q+1jlPB80\naJB62KS94z+DAwAAAAiOQQMAAABAcAwaAAAAAIJj0AAAAAAQHIMGAAAAgOAYNAAAAAAEp369bVpU\nVVW5mWnTpkm1br/9djczdepUqVYcTU1NSb8m8J133pHqvPzyy27mt99+k2pt2rTJzaxZs0aqpXyN\nX0egfOWbmdnzzz8f7Jg333xzsFqt9d1337mZWbNmSbWUr9XbsmWLVOvss892M4899ljS+9WveY5L\neX36/PPPpVrl5eVuZvTo0VKtE088UcqlSm1trdv7P/74w62ze/du6Xh79uxxM0OGDJFqHXbYYVIu\n06k7fJSvrl60aJFU6/TTT3czo0aNkmqlk7Kb65FHHpFqPfvss24m8NfbppXSO+W9mJnZCSec4GaU\nr1TuTJTrxPnnny/VUnJz5syRann+f/2WAAAAALQLBg0AAAAAwTFoAAAAAAiOQQMAAABAcAwaAAAA\nAIJj0AAAAAAQHIMGAAAAgOAYNAAAAAAEl5aFfVEUSbl58+a5mZ9//lmqpSwTSqXm5uakC/vWrl0r\n1Tn11FPdzA8//BCsVr9+/aRanYXau1tuucXN/PTTT219OCmnLGQsKyuTat10001uRjnnzMymTJni\nZvLy8pLen5OTIx0rrnXr1rmZ5557TqqlLP+74YYbpFqbN292MyUlJVKtOD766CN3aegrr7zi1unR\no4d0PGXJZmVlpVRr6dKlbuboo4+WaqVKsuvIf9RFW71793Yzr7/+ulRLec52BB9++KGbURYdmmlL\nJ+vr66Vaubm5Ui4V1EW2L730kpvp37+/VEu5nmRnZ0u10kl5v6ssHTUzW7x4sZvZvn27VEv9nYbA\nJxoAAAAAgmPQAAAAABAcgwYAAACA4Bg0AAAAAATHoAEAAAAgOAYNAAAAAMExaAAAAAAIjkEDAAAA\nQHAMGgAAAACCS8tm8D/++EPK3XPPPW5mwYIFUq1DDz1UyqVKdnZ20k3Fd911l1RH6YmyGdjM7Omn\nn3YzHWHzpqqmpsbNnHvuuVKtq666ys0cc8wxUq10UrYvexu4/7NixQo3o56bykbtVatWJb1/w4YN\n0rHiOv30093MypUrpVrbtm1zM+PHj5dqdemS3r8/GjdunI0YMSJpRnmeKT0xM1u9erWbufzyy6Va\ny5YtczO33XabVCtVGhoa3MyiRYukWsproqq4uDhYrVRQNqqbmb377rtuRt3mPXLkSDezfPlyqdbE\niROlXCrs3btXyn3wwQdu5pRTTpFqKdfYu+++W6p1/PHHS7nWamxstMbGxqQZ5Xz67LPPpOOtWbPG\nzVRUVEi1zjrrLCkXAp9oAAAAAAiOQQMAAABAcAwaAAAAAIJj0AAAAAAQHIMGAAAAgOAYNAAAAAAE\nx6ABAAAAIDgGDQAAAADBpWVh3/PPPy/l+vbt62amT58u1YqiyM00NzcnvV9d+BNHIpGQctOmTXMz\nv/zyi1Tr5ptvdjPe4q3/LF26tMX71OVGcXm/t/888MADbmb79u1Srblz57qZdC9Oi6LIPe/HjBnj\n1lm3bp10vOrqajejLqb8+OOP3UxRUVHS+5VlhJlCWXR12mmnSbX69evX1ofTJvn5+datW7ekmU8+\n+cSt88wzz0jHO+SQQ9xMr169pFr79u2TcumkLNB84403pFqTJk1yM+rvId1LcT3q7/abb75xM7t2\n7ZJq1dXVuZmO8Dqlvj9RrgGPPPKIVKugoMDNzJ49W6qVqoV9lZWVVl5enjTz6quvunWU56GZ2Zdf\nfulmevbsKdVSFn+GwicaAAAAAIJj0AAAAAAQHIMGAAAAgOAYNAAAAAAEx6ABAAAAIDgGDQAAAADB\nMWgAAAAACI5BAwAAAEBwDBoAAAAAggu+GVzZAv3RRx9JtR5++GE3o2yFNdM2g3sZpUZc6pbG5cuX\nu5nffvtNqlVVVeVmdu7cKdVKtp1b3dwd18aNG6Xc4sWL3cwXX3wh1TrssMOkXDo1NDS4z8c777zT\nraNs6TbTtqoXFhZKtd566y03U1pamvT+2tpa6VipVFNTI+WeeuopN/Pcc89JtdK9kV5x7LHHupmX\nXnpJqqVcczZs2CDVOvvss6VcOilbmvv37y/VGjVqlJu56KKLpFrq9uh0yc7OlnIzZ850M+p7mIUL\nF7oZ5XeQbsqWbjOzuXPnuplVq1ZJtc444ww3M2HCBKlWquTn57vXNGVTt/q+Q9mqrr4fysnJkXIh\nZP4VCQAAAECHw6ABAAAAIDgGDQAAAADBMWgAAAAACI5BAwAAAEBwDBoAAAAAgmPQAAAAABAcgwYA\nAACA4IIv7FOWgLz++utSrb59+7b14RygLBPKykreDu/+9nDEEUe4mSuvvFKqNXToUDczfPhwqVa3\nbt1avC8/P1+qEVdJSYmUU5biqLU6gpycHMvNzU2amTNnjltn7Nix0vGKi4vdzEknnSTVUp773nM6\nExaIbd26VcoNGDDAzYwYMaKtDydj9O7dO1itNWvWuJnZs2dLtYYNG9bWh5MRioqKpNz8+fPdTMjr\ncDp17dpVyt1xxx1u5tZbb5VqtedStFRSX0unTJkSJNNR9OrVy13e++KLLwY7nrJ0cvTo0cGOFwqf\naAAAAAAIjkEDAAAAQHAMGgAAAACCY9AAAAAAEByDBgAAAIDgGDQAAAAABMegAQAAACA4Bg0AAAAA\nwakb6PLMzMrKytxgFEVuprKyUjqosuwqLy9PqhXCQX/+1hxU6t2+ffukYuXl5W6mqalJqqUs4VF+\nn2ZmBQUFLd4Xs28H8l7vamtrpWI7d+50M9u2bZNqtZe2nHPr1693g1u2bHEzyjlnZlZdXe1mvAWC\n/1GW/3nnbyqfr6qKigopp/ROWUxnpi118mRC71TKeb5582ap1nfffedmvMWtmdC77du3B8up14AQ\nMqF3yvWzublZqhXiuahI9TW2M0v1OdfY2Nj6B9UCZWl0yOdrsGtsFEXuzcxmmFnE7cBthtI3ehe/\nb/Qufu/oG72jdxlxo3f0LmP7Ru/i946+ta53CWX6SSQSxWY22czKzazO/YHOK8/MBprZyiiKdig/\nQO/MLEbfzOjdfpxz8dG7+OhdfPQuPnoXD9fY+Djn4pN6Jw0aAAAAANAa/GdwAAAAAMExaAAAAAAI\njkEDAAAAQHAMGgAAAACCY9AAAAAAEByDBgAAAIDgGDQAAAAABPc/5TjgV8nqmaEAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bbccda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# *** VISUALIZATION 20 RANDOM TRAINING SAMPLES ***\n",
    "# Create 20 subplots\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(10):\n",
    "        axes[i][j].imshow(Xtrain_down[np.random.randint(0, 6000),:].reshape(int(xdim*factor), \n",
    "                          int(ydim*factor)), cmap='gray_r', interpolation='nearest')\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elahe/anaconda3/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/elahe/anaconda3/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873\n",
      "0.8644\n"
     ]
    }
   ],
   "source": [
    "# # *** LDA ***\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# # Original images\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(Xtrain, Ytrain)\n",
    "# LinearDiscriminantAnalysis(n_components=9, priors=None, shrinkage=None,\n",
    "#               solver='svd', store_covariance=False, tol=0.0001)\n",
    "\n",
    "# # Returns the mean accuracy on the given test data and labels.\n",
    "# score = clf.score(Xtest, Ytest, sample_weight=None)\n",
    "# print(score)\n",
    "\n",
    "# # Down-sampled images\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(Xtrain_down, Ytrain)\n",
    "# LinearDiscriminantAnalysis(n_components=9, priors=None, shrinkage=None,\n",
    "#               solver='svd', store_covariance=False, tol=0.0001)\n",
    "\n",
    "# # Returns the mean accuracy on the given test data and labels.\n",
    "# score = clf.score(Xtest_down, Ytest, sample_weight=None)\n",
    "# print(score)\n",
    "# # print(clf.predict(Xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elahe/anaconda3/lib/python3.5/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 15s - loss: 1.8134 - acc: 0.3032    \n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 14s - loss: 1.1683 - acc: 0.5814    \n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 14s - loss: 1.0605 - acc: 0.6158    \n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.9991 - acc: 0.6431    \n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.9194 - acc: 0.6736    \n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.8046 - acc: 0.7160    \n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.6792 - acc: 0.7742    \n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.6215 - acc: 0.7994    \n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.5842 - acc: 0.8129    \n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.5402 - acc: 0.8288    \n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.5028 - acc: 0.8408    \n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.4774 - acc: 0.8496    \n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.4548 - acc: 0.8569    \n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.4347 - acc: 0.8627    \n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.4145 - acc: 0.8698    \n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3917 - acc: 0.8764    \n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3746 - acc: 0.8813    \n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.3607 - acc: 0.8862    \n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3469 - acc: 0.8899    \n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3407 - acc: 0.8917    \n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3289 - acc: 0.8951    \n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3221 - acc: 0.8970    \n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3140 - acc: 0.8999    \n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3079 - acc: 0.9014    \n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.3001 - acc: 0.9042    \n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2977 - acc: 0.9052    \n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2891 - acc: 0.9073    \n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2840 - acc: 0.9097    \n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2791 - acc: 0.9110    \n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2712 - acc: 0.9143    \n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2675 - acc: 0.9140    \n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 13s - loss: 0.2629 - acc: 0.9147    \n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2588 - acc: 0.9170    \n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2547 - acc: 0.9184    \n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2472 - acc: 0.9201    \n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2451 - acc: 0.9221    \n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2407 - acc: 0.9231    \n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 15s - loss: 0.2376 - acc: 0.9227    \n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 16s - loss: 0.2343 - acc: 0.9256    \n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2317 - acc: 0.9267    \n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2256 - acc: 0.9276    \n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2268 - acc: 0.9270    \n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2229 - acc: 0.9285    \n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2198 - acc: 0.9292    \n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2186 - acc: 0.9300    \n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2158 - acc: 0.9317    \n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2112 - acc: 0.9320    \n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2106 - acc: 0.9318    \n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2083 - acc: 0.9328    \n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 14s - loss: 0.2054 - acc: 0.9335    \n",
      "\n",
      "acc: 94.12%\n"
     ]
    }
   ],
   "source": [
    "# *** TRAIN A FULLY-CONNECTED NN WITH TWO HIDDEN LAYERS ***\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(int(xdim*factor)**2,), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(Xtrain_down, Ytrain, nb_epoch=50, batch_size=16)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(Xtest_down, Ytest, verbose=0)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# # calculate predictions\n",
    "# Ypredict = model.predict(Xtest)\n",
    "# # round predictions\n",
    "# rounded = [round(x[0]) for x in Ypredict]\n",
    "# print(rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.023101  ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.03259186],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.01482217],\n",
      "       ..., \n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.01421584],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.01554491]], dtype=float32), array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.06885444,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.43515649,  0.10050378],\n",
      "       [ 0.02554294,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.23359326,  0.0940301 ],\n",
      "       ..., \n",
      "       [ 0.19536428,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.12916145,  0.03500222],\n",
      "       [ 0.15678462,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.11911228,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.26854488,  0.22753656]], dtype=float32), array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.39481941],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  1.46803093],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ..., \n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.64285648],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  1.44292665]], dtype=float32), array([[  1.34049333e-06,   4.25047489e-07,   6.36389770e-04, ...,\n",
      "          9.97962594e-01,   1.13594515e-05,   6.07864466e-04],\n",
      "       [  3.27722454e-08,   4.51357849e-03,   9.94393587e-01, ...,\n",
      "          1.95112852e-06,   1.27635712e-05,   6.32643826e-10],\n",
      "       [  2.00732466e-05,   9.97654140e-01,   1.04252214e-03, ...,\n",
      "          2.00772745e-04,   5.11452730e-04,   3.05363210e-05],\n",
      "       ..., \n",
      "       [  2.33938735e-07,   1.74275261e-09,   5.15125339e-06, ...,\n",
      "          2.32428225e-04,   3.97925178e-04,   3.01803313e-02],\n",
      "       [  4.73994123e-06,   1.20617015e-05,   2.18176108e-08, ...,\n",
      "          4.76892339e-08,   9.90362093e-03,   8.58462741e-08],\n",
      "       [  1.52192224e-04,   1.64621490e-08,   9.40598984e-05, ...,\n",
      "          1.14659009e-07,   3.95457646e-05,   7.29036298e-09]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# GET THE OUTPUT OF EACH LAYER AFTER TRAINING\n",
    "from keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "#functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "\n",
    "# Testing\n",
    "layer_outs = functor([Xtest_down, 1.])\n",
    "print(layer_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
