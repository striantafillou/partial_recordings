{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = 'DNN'\n",
    "\n",
    "import pickle\n",
    "with open('results/' + nn + '_layer_outputs.dat', 'rb') as f:\n",
    "    layer_outs,layer_outs_test= pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load recording and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from scipy.misc import imresize\n",
    "\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n",
    "\n",
    "# Recording data\n",
    "X_pr = Xtrain[30000:60000, :, :]\n",
    "Y_pr = Ytrain[30000:60000]\n",
    "\n",
    "# downsample\n",
    "factor = 1\n",
    "if factor<1:\n",
    "    Xtest_down = np.ones((Xtest.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "    for i in range(Xtest.shape[0]):\n",
    "        Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n",
    "\n",
    "    X_pr_down = np.ones((X_pr.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "    for i in range(X_pr.shape[0]):\n",
    "        X_pr_down[i,:,:] = imresize(X_pr[i,:,:], factor)\n",
    "else:\n",
    "    Xtest_down = Xtest\n",
    "    X_pr_down = X_pr\n",
    "    \n",
    "# VECTORIZE IMAGES\n",
    "Xtest_down = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "X_pr_down = X_pr_down.reshape(X_pr_down.shape[0], int(xdim*factor)**2).astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Neural Networks and average outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layer #', 0)\n",
      "1\n",
      "680.257632971\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10) into shape (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-36b874d8a405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mrmses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_new\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlayer_outs_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moLayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'predicting, mean RMSEs=%f'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10) into shape (5)"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "from RE_PartialRecData2 import RE_PartialRecData2\n",
    "from sklearn import preprocessing, linear_model\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "#from keras import backend as K\n",
    "import time\n",
    "\n",
    "# how many recordings?\n",
    "nRecordings = 10\n",
    "# which layers?\n",
    "layers = [0,2,5]\n",
    "# how many iterations\n",
    "nIterations = 5\n",
    "\n",
    "# baseline prediction error\n",
    "#bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "oLayer = len(layer_outs)-1  # index of output layer\n",
    "nOutNeurons = layer_outs[oLayer].shape[1]\n",
    "\n",
    "for iLayer in layers:\n",
    "    # how many neurons from the firs hidden layer?\n",
    "    subnetSize = [2**x for x in range(np.int(np.log2(layer_outs[iLayer].shape[1])+1))]\n",
    "    nSubnetSize = len(subnetSize)\n",
    "    # how many samples per recording?\n",
    "    nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)*100\n",
    "    \n",
    "    rmses = np.zeros([nIterations, nOutNeurons, nSubnetSize])\n",
    "    print('Layer #', iLayer)\n",
    "    \n",
    "    for ss in range(nSubnetSize):\n",
    "        nLayerNeurons = subnetSize[ss]\n",
    "        print(subnetSize[ss]) \n",
    "        \n",
    "        for it in range(nIterations):\n",
    "            start = time.time()\n",
    "            # copy data - is this necessary?\n",
    "            layer_outputs = deepcopy(layer_outs)          \n",
    "            # subsample\n",
    "            X_subsample, Y_subsample = RE_PartialRecData2(layer_outputs[iLayer], layer_outputs[oLayer], \n",
    "                                                          nLayerNeurons, nRecordings, nSamples[ss])\n",
    "            \n",
    "            # impute X_subsample with mean value, apply imputation to test set\n",
    "            imp = preprocessing.Imputer(missing_values='NaN', strategy='mean')\n",
    "            impf = imp.fit(X_subsample)\n",
    "            X_new = impf.transform(X_subsample)\n",
    "            X_test_new = impf.transform(layer_outs_test[iLayer])\n",
    "            \n",
    "            # fit ΝΝ\n",
    "            model = Sequential()\n",
    "            model.add(Dense(16, input_shape=(X_new.shape[1],), activation='relu'))\n",
    "            model.add(Dense(16, activation='relu'))\n",
    "            model.add(Dense(16, activation='relu'))                \n",
    "            model.add(Dense(10, activation='softmax'))\n",
    "            \n",
    "            # Compile model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=0)]\n",
    "            model.fit(X_new, Y_subsample, epochs=30, batch_size=16, shuffle=True, verbose=0, \n",
    "                      validation_split=0.1, callbacks=callbacks)\n",
    "            stop = time.time()\n",
    "            duration = stop-start\n",
    "            print(duration)\n",
    "            rmses[it,:,ss] = np.sqrt(np.mean((model.predict(X_test_new) - layer_outs_test[oLayer])**2, axis=0))\n",
    "        \n",
    "        print ('predicting, mean RMSEs=%f' %np.mean(rmses[it, :, ss]))\n",
    "        \n",
    "        \n",
    "    fName = 'results/NNMVI_RMSES_Layer' + str(iLayer) + '_nRec' + str(nRecordings) + '_' + nn + '.dat'\n",
    "    with open(fName,'wb') as f:\n",
    "        pickle.dump(rmses, f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
