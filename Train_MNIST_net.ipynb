{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# Load MNIST dataset\n",
    "from keras.datasets import mnist\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n",
    "\n",
    "# split train data in two\n",
    "X_pr = Xtrain[30000:60000, :, :]\n",
    "Y_pr = Ytrain[30000:60000]\n",
    "\n",
    "Xtrain = Xtrain[0:30000, :, :];\n",
    "Ytrain = Ytrain[0:30000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *** DOWNSAMPLE THE IMAGES ***\n",
    "factor = 0.25\n",
    "\n",
    "Xtrain_down = np.ones((Xtrain.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    Xtrain_down[i, :, :] = imresize(Xtrain[i,:,:], factor)\n",
    "\n",
    "Xtest_down = np.ones((Xtest.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(Xtest.shape[0]):\n",
    "    Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n",
    "\n",
    "    \n",
    "X_pr_down = np.ones((X_pr.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "for i in range(X_pr.shape[0]):\n",
    "    X_pr_down[i,:,:] = imresize(X_pr[i,:,:], factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *** VECTORIZE IMAGES ***\n",
    "Xtrain_down = Xtrain_down.reshape(Xtrain.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtest_down  = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "X_pr_down   = X_pr_down.reshape(X_pr.shape[0], int(xdim*factor)**2).astype('float32') / 255\n",
    "Xtrain      = Xtrain.reshape(Xtrain.shape[0], xdim**2).astype('float32') / 255\n",
    "Xtest       = Xtest.reshape(ntest, xdim**2).astype('float32') / 255\n",
    "# Categorical labels\n",
    "Ytrain = np_utils.to_categorical(Ytrain, 10)\n",
    "Ytest = np_utils.to_categorical(Ytest, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAACqCAYAAAAqYjm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHHVTSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13483d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# *** VISUALIZATION 20 RANDOM TRAINING SAMPLES ***\n",
    "# Create 20 subplots\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(10):\n",
    "        axes[i][j].imshow(Xtrain_down[np.random.randint(0, 3000),:].reshape(int(xdim*factor), \n",
    "                          int(ydim*factor)), cmap='gray_r', interpolation='nearest')\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 49)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # *** LDA ***\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# # Original images\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(Xtrain, Ytrain)\n",
    "# LinearDiscriminantAnalysis(n_components=9, priors=None, shrinkage=None,\n",
    "#               solver='svd', store_covariance=False, tol=0.0001)\n",
    "\n",
    "# # Returns the mean accuracy on the given test data and labels.\n",
    "# score = clf.score(Xtest, Ytest, sample_weight=None)\n",
    "# print(score)\n",
    "\n",
    "# # Down-sampled images\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf.fit(Xtrain_down, Ytrain)\n",
    "# LinearDiscriminantAnalysis(n_components=9, priors=None, shrinkage=None,\n",
    "#               solver='svd', store_covariance=False, tol=0.0001)\n",
    "\n",
    "# # Returns the mean accuracy on the given test data and labels.\n",
    "# score = clf.score(Xtest_down, Ytest, sample_weight=None)\n",
    "# print(score)\n",
    "# # print(clf.predict(Xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elahe/anaconda3/lib/python3.5/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30000/30000 [==============================] - 8s - loss: 1.8931 - acc: 0.2582     \n",
      "Epoch 2/50\n",
      "30000/30000 [==============================] - 7s - loss: 1.6495 - acc: 0.3505     \n",
      "Epoch 3/50\n",
      "30000/30000 [==============================] - 7s - loss: 1.6003 - acc: 0.3724     \n",
      "Epoch 4/50\n",
      "30000/30000 [==============================] - 8s - loss: 1.5600 - acc: 0.3846     \n",
      "Epoch 5/50\n",
      "30000/30000 [==============================] - 8s - loss: 1.4522 - acc: 0.4308     \n",
      "Epoch 6/50\n",
      "30000/30000 [==============================] - 8s - loss: 1.2546 - acc: 0.5220     \n",
      "Epoch 7/50\n",
      "30000/30000 [==============================] - 8s - loss: 1.0957 - acc: 0.5935     \n",
      "Epoch 8/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.9205 - acc: 0.6717     \n",
      "Epoch 9/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.7945 - acc: 0.7305     \n",
      "Epoch 10/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.7220 - acc: 0.7606     \n",
      "Epoch 11/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.6644 - acc: 0.7834     \n",
      "Epoch 12/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.5914 - acc: 0.8104     \n",
      "Epoch 13/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.5527 - acc: 0.8243     \n",
      "Epoch 14/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.5275 - acc: 0.8328     \n",
      "Epoch 15/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.5018 - acc: 0.8432     \n",
      "Epoch 16/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.4800 - acc: 0.8483     \n",
      "Epoch 17/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.4593 - acc: 0.8579     \n",
      "Epoch 18/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.4446 - acc: 0.8581     \n",
      "Epoch 19/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.4296 - acc: 0.8655     \n",
      "Epoch 20/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.4200 - acc: 0.8676     \n",
      "Epoch 21/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.4045 - acc: 0.8733     \n",
      "Epoch 22/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.3959 - acc: 0.8764     \n",
      "Epoch 23/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.3884 - acc: 0.8784     \n",
      "Epoch 24/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.3784 - acc: 0.8810     \n",
      "Epoch 25/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.3702 - acc: 0.8830     \n",
      "Epoch 26/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.3609 - acc: 0.8849     \n",
      "Epoch 27/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.3505 - acc: 0.8895     \n",
      "Epoch 28/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.3453 - acc: 0.8914     \n",
      "Epoch 29/50\n",
      "30000/30000 [==============================] - 11s - loss: 0.3358 - acc: 0.8937    \n",
      "Epoch 30/50\n",
      "30000/30000 [==============================] - 9s - loss: 0.3245 - acc: 0.8980     \n",
      "Epoch 31/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.3238 - acc: 0.8966     \n",
      "Epoch 32/50\n",
      "30000/30000 [==============================] - 9s - loss: 0.3164 - acc: 0.8981     \n",
      "Epoch 33/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.3093 - acc: 0.9013     \n",
      "Epoch 34/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.3071 - acc: 0.9037     \n",
      "Epoch 35/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2987 - acc: 0.9041     \n",
      "Epoch 36/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2914 - acc: 0.9074     \n",
      "Epoch 37/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2833 - acc: 0.9079     \n",
      "Epoch 38/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2855 - acc: 0.9095     \n",
      "Epoch 39/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2755 - acc: 0.9124     \n",
      "Epoch 40/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2752 - acc: 0.9116     \n",
      "Epoch 41/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2686 - acc: 0.9149     \n",
      "Epoch 42/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2657 - acc: 0.9152     \n",
      "Epoch 43/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2625 - acc: 0.9159     \n",
      "Epoch 44/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2570 - acc: 0.9181     \n",
      "Epoch 45/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2560 - acc: 0.9174     \n",
      "Epoch 46/50\n",
      "30000/30000 [==============================] - 8s - loss: 0.2513 - acc: 0.9190     \n",
      "Epoch 47/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2458 - acc: 0.9202     \n",
      "Epoch 48/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2449 - acc: 0.9205     \n",
      "Epoch 49/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2431 - acc: 0.9216     \n",
      "Epoch 50/50\n",
      "30000/30000 [==============================] - 7s - loss: 0.2394 - acc: 0.9243     \n",
      "\n",
      "acc: 92.35%\n"
     ]
    }
   ],
   "source": [
    "# *** TRAIN A FULLY-CONNECTED NN WITH TWO HIDDEN LAYERS ***\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(int(xdim*factor)**2,), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(Xtrain_down, Ytrain, nb_epoch=50, batch_size=16)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(Xtest_down, Ytest, verbose=0)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# # calculate predictions\n",
    "# Ypredict = model.predict(Xtest)\n",
    "# # round predictions\n",
    "# rounded = [round(x[0]) for x in Ypredict]\n",
    "# print(rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.02202845,  0.        ,  0.32602513, ...,  0.11971657,\n",
      "         0.        ,  3.90735912],\n",
      "       [ 0.        ,  0.        ,  0.3178001 , ...,  0.01852087,\n",
      "         0.        ,  0.71622854],\n",
      "       [ 0.18396944,  0.        ,  0.46906987, ...,  0.18578941,\n",
      "         0.        ,  3.42192411],\n",
      "       ..., \n",
      "       [ 0.05356272,  0.        ,  0.27201772, ...,  0.02083855,\n",
      "         0.        ,  4.19081211],\n",
      "       [ 0.        ,  0.        ,  0.19928239, ...,  0.0611527 ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.23968501, ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32), array([[  2.7717042 ,  11.38095188,   8.93695259, ...,   0.        ,\n",
      "          0.        ,   7.36273623],\n",
      "       [  0.        ,   3.8440032 ,  18.38563538, ...,   0.16622734,\n",
      "          0.        ,   3.1886692 ],\n",
      "       [  2.14769006,   7.95462322,  17.96408844, ...,   0.        ,\n",
      "          0.        ,   7.72371817],\n",
      "       ..., \n",
      "       [ 11.93557549,  13.88940144,   0.        , ...,   0.        ,\n",
      "          0.        ,   1.59990537],\n",
      "       [ 13.27334118,   0.        ,   0.        , ...,   0.15421379,\n",
      "         12.53028393,   0.        ],\n",
      "       [ 10.64247513,  14.00748062,  22.31798363, ...,   0.        ,\n",
      "          0.        ,   1.55679011]], dtype=float32), array([[   0.        ,  190.75239563,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    2.9031291 ,    4.01677942, ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,  192.00434875,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       ..., \n",
      "       [   0.        ,  205.04066467,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,  106.84342194,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ],\n",
      "       [   0.        ,    9.45107269,    0.        , ...,    0.        ,\n",
      "           0.        ,    0.        ]], dtype=float32), array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   0.00000000e+00,   7.98491016e-21, ...,\n",
      "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
      "       ..., \n",
      "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
      "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# GET THE OUTPUT OF EACH LAYER AFTER TRAINING\n",
    "from keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "#functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "\n",
    "# Testing\n",
    "layer_outs = functor([X_pr_down, 1.])\n",
    "print(layer_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
