{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = 'NN'\n",
    "\n",
    "import pickle\n",
    "with open('results/' + nn + '_layer_outputs.dat','rb') as f:\n",
    "    layer_outs,layer_outs_test= pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load recording and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from scipy.misc import imresize\n",
    "\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = mnist.load_data()\n",
    "\n",
    "(ntrain, xdim, ydim) = Xtrain.shape\n",
    "ntest = Xtest.shape[0]\n",
    "\n",
    "# Recording data\n",
    "X_pr = Xtrain[30000:60000, :, :]\n",
    "Y_pr = Ytrain[30000:60000]\n",
    "\n",
    "\n",
    "# downsample\n",
    "factor = 1\n",
    "if factor<1:\n",
    "    Xtest_down = np.ones((Xtest.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "    for i in range(Xtest.shape[0]):\n",
    "        Xtest_down[i,:,:] = imresize(Xtest[i,:,:], factor)\n",
    "\n",
    "    X_pr_down = np.ones((X_pr.shape[0], int(xdim*factor), int(ydim*factor)))\n",
    "    for i in range(X_pr.shape[0]):\n",
    "        X_pr_down[i,:,:] = imresize(X_pr[i,:,:], factor)\n",
    "else:\n",
    "    Xtest_down = Xtest\n",
    "    X_pr_down = X_pr\n",
    "    \n",
    "# VECTORIZE IMAGES\n",
    "Xtest_down = Xtest_down.reshape(ntest, int(xdim*factor)**2).astype('float32') / 255\n",
    "X_pr_down = X_pr_down.reshape(X_pr_down.shape[0], int(xdim*factor)**2).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[SoftImpute] Max Singular Value of X_init = 1432.998201\n",
      "[SoftImpute] Iter 1: observed MAE=0.019742 rank=10\n"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "from RE_PartialRecData import RE_PartialRecData\n",
    "from RE_PartialRecData2 import RE_PartialRecData2\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from fancyimpute import SoftImpute as FI #SimpleFill, KNN, NuclearNormMinimization, SoftImpute\n",
    "import pickle\n",
    "\n",
    "\n",
    "params = {}\n",
    "# use softmax multi-class classification 'multi:softmax'\n",
    "# use linear regression 'reg:linear'\n",
    "params['objective'] = 'reg:linear'\n",
    "# scale weight of positive examples\n",
    "params['eta'] = 0.5               # Makes the model more robust by shrinking the weights on each step (0.01-0.2)\n",
    "params['max_depth'] = 6           # Used to control over-fitting as higher depth will allow model to learn relations \n",
    "                                  # very specific to a particular sample. (3-10)\n",
    "params['silent'] = 1\n",
    "params['nthread'] = 4\n",
    "# params['num_class'] = 10\n",
    "num_round = 5\n",
    "\n",
    "# how many recordings?\n",
    "recordings = [10]\n",
    "nRecordings = len(recordings)\n",
    "# which layers?\n",
    "layers = [0,2]\n",
    "# how many iterations\n",
    "nIterations = 5\n",
    "\n",
    "# baseline prediction error\n",
    "#bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "oLayer = len(layer_outs)-1  # index of output layer\n",
    "nOutNeurons = layer_outs[oLayer].shape[1]\n",
    "\n",
    "for iLayer in layers:\n",
    "    # how many neurons from the firs hidden layer?\n",
    "    subnetSize = [2**x for x in range(np.int(np.log2(layer_outs[iLayer].shape[1])+1))]\n",
    "    nSubnetSize = len(subnetSize)\n",
    "    # how many samples per recording?\n",
    "    nSamples = np.divide(int(X_pr_down.shape[0]/nRecordings),subnetSize)*100\n",
    "    \n",
    "    rmses = np.zeros([nOutNeurons, nIterations, nSubnetSize, nRecordings])\n",
    "    for nr in range(nRecordings):\n",
    "        # how many samples per recording?\n",
    "        nSamples = np.divide(int(X_pr_down.shape[0]/recordings[nr]),subnetSize)*100\n",
    "        for ss in range(nSubnetSize):\n",
    "            print(subnetSize[ss])\n",
    "            for it in range(nIterations):\n",
    "                # copy data\n",
    "                layer_outputs = deepcopy(layer_outs)\n",
    "                # subsample\n",
    "                X_subsample, Y_subsample = RE_PartialRecData2(layer_outputs[iLayer], layer_outputs[oLayer], \n",
    "                                                              subnetSize[ss], recordings[nr],nSamples[ss])\n",
    "                if ss==nSubnetSize-1:\n",
    "                    X_impute = X_subsample\n",
    "                else:\n",
    "                    X_impute = FI(convergence_threshold=0.01, max_iters=30).complete(X_subsample)\n",
    "\n",
    "        #        print('# nan values: ',np.count_nonzero(np.isnan(X_subsample)))\n",
    "                # prepare data for xgboost\n",
    "                for iN in range(nOutNeurons):\n",
    "                    print('#neuron, #iteratin, subnetsize: ', iN,it,subnetSize[ss])\n",
    "                    xg_train  = xgb.DMatrix(X_impute, label=Y_subsample[:, iN])\n",
    "                    xg_test   = xgb.DMatrix(layer_outs_test[iLayer], label=layer_outs_test[oLayer][:,iN])\n",
    "                    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "                    # train XGboost\n",
    "                    bst = xgb.train(params, xg_train, num_round, watchlist, verbose_eval=False)\n",
    "                    # get predictions\n",
    "                    pred = bst.predict(xg_test)\n",
    "                    rmses[iN,it,ss,nr] = np.sqrt(np.mean(np.square([(pred[i] - layer_outs_test[oLayer][:,iN][i]) \n",
    "                                                 for i in range(len(layer_outs_test[oLayer][:,1]))])))\n",
    "                print ('predicting, RMSE=%f' %np.mean(rmses[:, it, ss, nr]))\n",
    "\n",
    "    # save the rmse's\n",
    "    fName = 'results/softImpute_XGB_RMSES_Layer' + str(iLayer) + '_nRec' + str(nRecordings) + '_' + nn + '.dat'\n",
    "    with open(fName,'wb') as f:\n",
    "        pickle.dump(rmses, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot RMSE's vs. No. observed neurons per recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as pl\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "\n",
    "# fig=pl.figure(figsize=(10,6))\n",
    "# ax1 = fig.add_subplot(111)\n",
    "# ax1.set_xlim([0, 128])\n",
    "# ax2 = ax1.twiny()\n",
    "\n",
    "# x = subnetSize;\n",
    "# y = np.mean(np.median(rmses, axis=1), axis=0)\n",
    "# error = np.std(np.mean(rmses, axis=1), axis=0)\n",
    "# bl = np.std(layer_outs_test[oLayer]-np.mean(layer_outs_test[oLayer]));\n",
    "\n",
    "# pl.semilogx(x, y, 'k-')\n",
    "# # horiz_line_data = np.array([bl for i in xrange(len(x))])\n",
    "# # pl.plot(x, horiz_line_data, 'k--') \n",
    "# pl.fill_between(x, y-error, y+error, alpha=0.2, facecolor='#808080')\n",
    "\n",
    "# ax1.set_xlabel('# observed neurons on Layer' + str(iLayer) + '(out of 128)', fontsize=18)\n",
    "# ax2.set_xlabel('Samples per recording',  fontsize=16)\n",
    "\n",
    "# new_tick_locations = subnetSize\n",
    "# ax1.set_xlim(ax1.get_xlim())\n",
    "# ax1.set_xticks(new_tick_locations)\n",
    "# ax1.set_xticklabels(new_tick_locations)\n",
    "\n",
    "# new_tick_locations = subnetSize\n",
    "# ax2.set_xlim(ax1.get_xlim())\n",
    "# ax2.set_xticks(new_tick_locations)\n",
    "# ax2.set_xticklabels(nSamples)\n",
    "\n",
    "\n",
    "# # ax2.set_xticks(nSamples)\n",
    "# # ax2.set_xticklabels(nSamples[range(0, 5, 20)])\n",
    "# ax1.set_ylabel('RMSE', fontsize=18)\n",
    "# ax1.set_ylim([0, .50])\n",
    "\n",
    "# # pl.text(110,bl+0.005, 'baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
